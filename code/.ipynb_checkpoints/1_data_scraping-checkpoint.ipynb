{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Assembly DSI 13 EC \n",
    "# Project 3 - Web APIs & NLP\n",
    "## Mike Bell \n",
    "### October 23, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "The goal of this project is to use NLP and various classification models to predict if a reddit post came from r/math or r/physics. This classification will be done on the text content of the post alone (title and body), no comments or pictures/media are used. \n",
    "\n",
    "Most of the code here can be very easily modified for use on any two subreddits: each notebook contains a 'subreddit' variable which is a list containing the names of the two subreddits to be compared. Most of the data scraping, preprocessing and cleaning can be done by simply changing the names of the desired subreddits, and running the notebooks. The data directory '../data/subreddit0_subreddit1_data/' is used to save and retrieve the csv files generated during the scraping and preprocessing stages. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 1: Data Collection using the Pushshift Reddit API\n",
    "\n",
    "In this notebook we use the PushShift Reddit API to scrape and collect data from two subreddits. \n",
    "Our primary goal is to train a classification model to predict which subreddit a submission came from based on its text (both title and body text).\n",
    "\n",
    "Our primary analysis will focus on the r/math and r/physics subreddits, but as stated above, the code below is set up so as to easily, and quickly, scrape data from any two given subreddits.\n",
    "\n",
    "Since Pushshift only allows 100 results per request, our scraping process pulls 100 items and tracks the UTC created timestamp of the oldest post retrieved. On the next request, we use this timestamp as the 'before' attribute to get another 100 posts older than this one. This process is repeated until the desired number of requests is made. \n",
    "\n",
    "Each request for each subreddit is written to a csv, no aggregation or cleaning is performed in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission endpoint for pushshift API\n",
    "SUB_ENDPT = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls submissions from a given subreddit, with parameters\n",
    "# sub_size (int): maximum number of posts to retrieve (limited by 100)\n",
    "# before (int): only look for posts created before this UTC timestamp \n",
    "# after (int): only look for posts created after this UTC timestamp\n",
    "# Note: We also choose to automatically ignore any posts with selftext equal to '[removed]', which typically indicates\n",
    "# a post was automoderated for being inappropriate for the given subreddit.\n",
    "\n",
    "def get_subreddit(subreddit, sub_size = 100, before = '', after = ''):\n",
    "   \n",
    "    sub_params = {    \n",
    "        'subreddit': subreddit,\n",
    "        'size' : sub_size, \n",
    "        'before' : before,\n",
    "        'after' : after,\n",
    "        'selftext:not' : '[removed]'\n",
    "    }\n",
    "\n",
    "    res = requests.get(SUB_ENDPT, sub_params)\n",
    "    \n",
    "    subs = pd.DataFrame(res.json()['data'])\n",
    "    \n",
    "    return subs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders and filenames are automatically generated based on the two subreddit names given in the list 'subreddits'\n",
    "subreddits = [ 'math', 'physics']\n",
    "subreddit_dir = f'../data/{subreddits[0]}_{subreddits[1]}_data/'\n",
    "if not os.path.exists(subreddit_dir):\n",
    "    os.makedirs(subreddit_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Iterate and make requests to each subreddit\n",
    "# the lowest utc timestamp for each previous request is tracked and used as the 'before' parameter \n",
    "# for the next request\n",
    "\n",
    "num_requests = 1000\n",
    "\n",
    "curr_time = int(datetime.now(timezone.utc).timestamp())\n",
    "oldest_utc = [curr_time,curr_time]\n",
    "for i in range(num_requests):\n",
    "    for idx, subr in enumerate(subreddits):\n",
    "        df = get_subreddit(subr, before = oldest_utc[idx])\n",
    "        oldest_utc[idx] = df['created_utc'].min()\n",
    "        df.to_csv(f'{subreddit_dir}{subr}_{i}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
