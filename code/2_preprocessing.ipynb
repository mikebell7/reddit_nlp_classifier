{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Assembly DSI 13 EC \n",
    "# Project 3 - Web APIs & NLP\n",
    "## Mike Bell \n",
    "### October 23, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 2: Data Preprocessing\n",
    "\n",
    "In this notebook, we aggregate, clean, and process the subreddit text post scrapes obtained in Notebook 1. \n",
    "\n",
    "In particular, we delete any empty posts, removed posts, and stickied posts. Most reddit posts seem to have no body text ('selftext') and we replace these missing values with empty strings. \n",
    "\n",
    "We also remove any capitalization, html generated symbols (ex: `&amp;`), hyperlinks, common stopwords etc.\n",
    "\n",
    "Finally NLP packages are used to lemmatize the combined title/body text, and the data is saved to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For NLP operations such as tokenization, lemmatization, and stopword removal\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate scraped dataframes into two large dataframes based on subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [ 'math', 'physics']\n",
    "\n",
    "subreddit_dir = f'../data/{subreddits[0]}_{subreddits[1]}_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files = 12 # how many files to include in aggregation\n",
    "\n",
    "# some subreddits include many spam/removed posts, so there will be an inbalance in the \n",
    "# number of cleaned examples in the dataset. We use the following parameters to include extra\n",
    "# files to balance the dataset. \n",
    "extra_files = 6\n",
    "unbalanced_indx = 0   # which subreddit (0 or 1) to add the extra files to\n",
    "\n",
    "# two lists to track each csv as a dataframe, will be combined at the end\n",
    "subreddit_dfs = [[],[]]\n",
    "\n",
    "for i in range(num_files):\n",
    "    for idx, sub in enumerate(subreddits): \n",
    "        subreddit_dfs[idx].append(pd.read_csv(f'{subreddit_dir}{sub}_{i}.csv'))\n",
    "\n",
    "\n",
    "# read in extra files, if needed\n",
    "for i in range(num_files, num_files + extra_files):\n",
    "    subreddit_dfs[unbalanced_indx].append(pd.read_csv(f'{subreddit_dir}{subreddits[unbalanced_indx]}_{i}.csv'))\n",
    "\n",
    "# combine into two dataframes\n",
    "df_0 = pd.concat(subreddit_dfs[0], axis = 0)\n",
    "df_1 = pd.concat(subreddit_dfs[1], axis = 0)\n",
    "\n",
    "df_0.reset_index(inplace = True, drop = True)\n",
    "df_1.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# write aggregated dataframes to files\n",
    "df_0.to_csv(f'{subreddit_dir}{subreddits[0]}_agg.csv', index = False)\n",
    "df_1.to_csv(f'{subreddit_dir}{subreddits[1]}_agg.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data: Remove spam/deleted/stickied posts, fill NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletes any posts which have been flagged as being removed\n",
    "# also removes any missing a title, or stickied\n",
    "\n",
    "# Replace any NaNs in selftext with empty strings, and return only the text/subreddit columns\n",
    "def clean_reddit_data(df):\n",
    "    \n",
    "    df = df[df['removed_by_category'].isnull()] # delete any removed posts\n",
    "    df = df[~df['title'].isnull()] # delete empty titled posts\n",
    "    df = df[df['stickied'] == False] # delete stickied posts\n",
    "    df.fillna({'selftext': ''}, inplace = True) # replace NaN selftext with empty strings\n",
    "\n",
    "    return df[['title', 'selftext', 'subreddit']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a list of words to be removed \n",
    "# consists of all stopwords, and stopwords with punctuation (apostrophes) removed as these show up often\n",
    "remove_words = list(set(stopwords.words('english') +  \\\n",
    "                        [x.replace(\"'\", \"\") for x in stopwords.words('english')]))\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "# tokenize text and remove stopwords and any\n",
    "def clean_text(text):\n",
    "    text_tokens = tokenizer.tokenize(text)\n",
    "    return ' '.join([word for word in text_tokens if ((word not in remove_words) & (word.isalpha()))])\n",
    "\n",
    "\n",
    "# does all the steps mentioned above and returns a combined 'text' column consisting of all text in each post\n",
    "def process_text(df):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # convert to lowercase\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    df['selftext'] = df['selftext'].str.lower()\n",
    "    df['subreddit'] = df['subreddit'].str.lower()\n",
    "    \n",
    "    \n",
    "    # First remove html converted symbols and hyperlinks from title and selftext\n",
    "    df['title'] = df['title'].str.replace(r'&\\w*;', '')\n",
    "    df['selftext'] = df['selftext'].str.replace(r'&\\w*;', '') \n",
    "    df['title'] = df['title'].replace('http\\S+', '', regex=True).replace('www\\S+', '', regex=True)\\\n",
    "                 .replace('#\\S+', '', regex=True).replace('\\n\\n\\S+', '', regex=True)\n",
    "    df['selftext'] = df['selftext'].replace('http\\S+', '', regex=True).replace('www\\S+', '', regex=True)\\\n",
    "                 .replace('#\\S+', '', regex=True).replace('\\n\\n\\S+', '', regex=True)\n",
    "    \n",
    "    \n",
    "    # apply stopword/symbol remover\n",
    "    df['title'] = df['title'].map(clean_text)\n",
    "    df['selftext'] = df['selftext'].map(clean_text)\n",
    "    \n",
    "    # remove digits\n",
    "    df['title'] = df['title'].str.replace(r'\\d+', '')\n",
    "    df['selftext'] = df['selftext'].str.replace(r'\\d+', '')\n",
    "    \n",
    "    \n",
    "    # join title and selftext into a single feature\n",
    "    df['text'] = df['title'] + ' ' + df['selftext']\n",
    "    df = df[df['text'].str.strip() != '']  # check for any empty text entries\n",
    "    \n",
    "    return df[['title', 'selftext', 'text', 'subreddit']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = clean_reddit_data(df_0)\n",
    "df_1 = clean_reddit_data(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        0\n",
       "selftext     0\n",
       "subreddit    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = process_text(df_0)\n",
    "df_1 = process_text(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine to one dataframe\n",
    "df = pd.concat([df_0, df_1], axis = 0)\n",
    "df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# convert subreddit column to binary\n",
    "df['subreddit'] = df['subreddit'].map({subreddits[0] : 0, subreddits[1] : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1083\n",
       "0    1057\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check balance of classes, go back to start and adjust extra_files parameters if needed\n",
    "df.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to csv\n",
    "df.to_csv(f'{subreddit_dir}{subreddits[0]}_{subreddits[1]}_combined.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize\n",
    "def lem_text(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem/lem the text column, as this is the column we will be training models on \n",
    "df['lem_text'] = df['text'].apply(lem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['title', 'selftext', 'text', 'lem_text', 'subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        0\n",
       "selftext     0\n",
       "text         0\n",
       "lem_text     0\n",
       "subreddit    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "df.to_csv(f'{subreddit_dir}{subreddits[0]}_{subreddits[1]}_combined_lem.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2140, 5)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.506075\n",
       "0    0.493925\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
